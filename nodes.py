import os
import glob
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import tempfile
import numpy as np
from loguru import logger
from typing import Optional, Tuple, Any, List, Dict
import folder_paths
import random
import urllib.request
import zipfile
import tarfile
from pathlib import Path
from datetime import datetime
import shutil
import time
import math
from tqdm import tqdm
from accelerate import init_empty_weights
from transformers import AutoTokenizer, AutoModel, ClapTextModelWithProjection
import comfy.model_management as mm
import comfy.utils
from safetensors.torch import load_file

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

# Import ComfyUI video types
try:
    from comfy_api.input_impl import VideoFromFile
except ImportError:
    try:
        # Fallback to latest API location
        from comfy_api.latest._input_impl.video_types import VideoFromFile
    except ImportError:
        logger.warning("VideoFromFile not available, will return file paths only")
        VideoFromFile = None

# Add foley models directory to ComfyUI folder paths
foley_models_dir = os.path.join(folder_paths.models_dir, "foley")
if "foley" not in folder_paths.folder_names_and_paths:
    folder_paths.folder_names_and_paths["foley"] = ([foley_models_dir], folder_paths.supported_pt_extensions)

# Import the HunyuanVideo-Foley modules
try:
    from hunyuanvideo_foley.utils.model_utils import load_model
    from .utils import denoise_process_safely
    from .utils import feature_process_unified, extract_video_path, create_node_exit_values
    from hunyuanvideo_foley.utils.media_utils import merge_audio_video
    from .model_urls import get_model_url
    from .model_management import find_or_download, get_siglip_path, get_clap_path, get_model_dir
except ImportError as e:
    logger.error(f"Failed to import HunyuanVideo-Foley modules: {e}")
    logger.error("Make sure the HunyuanVideo-Foley package is installed and accessible")
    raise

class HunyuanVideoFoleyNode:
    """
    A node for generating audio using the HunyuanVideo-Foley model
    """
    # Class-level model storage for persistence
    _model_dict = None
    _cfg = None
    _device = None
    _model_path = None
    _memory_efficient = False  # Track memory mode

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text_prompt": ("STRING", {
                    "multiline": True,
                    "default": "footstep sound, impact, water splash",
                    "display": "textarea",
                    "placeholder": "Describe the audio you want to generate..."
                }),
                "guidance_scale": ("FLOAT", {
                    "default": 4.5,
                    "min": 1.0,
                    "max": 10.0,
                    "step": 0.1,
                    "display": "slider"
                }),
                "num_inference_steps": ("INT", {
                    "default": 50,
                    "min": 10,
                    "max": 200,
                    "step": 1,
                    "display": "slider"
                }),
                "sample_nums": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 4,
                    "step": 1,
                    "display": "slider"
                }),
                "seed": ("INT", {
                    "default": 42,
                    "min": 0,
                    "max": 2**32 - 1,
                    "display": "number"
                }),
            },
            "optional": {
                # Hybrid input support - either VIDEO or IMAGE
                "video": ("VIDEO",),
                "images": ("IMAGE",),  # Support for frame sequences from PR
                "fps": ("FLOAT", {
                    "default": 24.0,
                    "min": 1.0,
                    "max": 120.0,
                    "step": 0.1,
                    "tooltip": "Frames per second (only used with IMAGE input)"
                }),
                # Negative prompt from PR
                "negative_prompt": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "display": "textarea",
                    "placeholder": "Additional negative prompts (optional). Will be combined with built-in quality controls."
                }),
                # Output options
                "output_format": (["video_path", "frames", "both"], {
                    "default": "both",
                    "tooltip": "Choose output format: video file path only works with VIDEO input, frames only works with IMAGE input and VIDEO input"
                }),
                "output_folder": ("STRING", {
                    "default": "hunyuan_foley",
                    "multiline": False,
                    "display": "text",
                    "placeholder": "Subfolder name in ComfyUI/output/"
                }),
                "filename_prefix": ("STRING", {
                    "default": "foley_",
                    "multiline": False,
                    "display": "text",
                    "placeholder": "Prefix for output filename"
                }),
                # Memory optimization options
                "feature_extraction_batch_size": ("INT", {
                    "default": 0, "min": 0, "max": 128, "step": 2,
                    "tooltip": "Frames to process at once. Set to 0 for auto-detection based on VRAM. Default is 16 for >16GB VRAM."
                }),
                "syncformer_batch_size": ("INT", {
                    "default": 8, "min": 1, "max": 64, "step": 1,
                    "tooltip": "(Advanced) Internal batch size for the Syncformer model. Lower this if you still get OOM errors with long videos."
                }),
                "enable_profiling": ("BOOLEAN", {
                    "default": False,
                    "display": "checkbox",
                    "tooltip": "Enable detailed performance and memory logging in the console."
                }),
                "memory_efficient": ("BOOLEAN", {
                    "default": False,
                    "display": "checkbox",
                    "tooltip": "Enable memory-efficient mode: unloads models after generation and uses aggressive garbage collection"
                }),
                "cpu_offload": ("BOOLEAN", {
                    "default": False,
                    "display": "checkbox",
                    "tooltip": "Offload models to CPU when not in use (slower but saves VRAM)"
                }),
                "enabled": ("BOOLEAN", {
                    "default": True,
                    "display": "checkbox",
                    "tooltip": "Enable or disable the entire audio generation process. If false, returns a silent or null audio output."
                }),
                "silent_audio": ("BOOLEAN", {
                    "default": True,
                    "display": "checkbox",
                    "tooltip": "If true, returns a silent audio clip when disabled or on failure. If false, returns None."
                }),
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "AUDIO", "STRING")
    RETURN_NAMES = ("video_path", "video_frames", "audio", "status_message")
    FUNCTION = "generate_audio"
    CATEGORY = "HunyuanVideo-Foley"
    DESCRIPTION = "Generate synchronized audio for videos using HunyuanVideo-Foley model"

    @classmethod
    def setup_device(cls, device_type: str = "auto", device_id: int = 0):
        """Setup and return the appropriate device with memory optimization"""
        # Try to import ComfyUI's model management if available
        try:
            import comfy.model_management as mm
            device = mm.get_torch_device()
            logger.info(f"Using ComfyUI device: {device}")
            return device
        except:
            pass

        if device_type == "auto":
            if torch.cuda.is_available():
                device = torch.device(f"cuda:{device_id}")
                # Clear cache before loading
                torch.cuda.empty_cache()
                if hasattr(torch.cuda, 'reset_peak_memory_stats'):
                    torch.cuda.reset_peak_memory_stats(device)
            else:
                device = torch.device("cpu")
        else:
            device = torch.device(device_type)

        logger.info(f"Using device: {device}")
        return device

    @classmethod
    def load_models(cls, model_path: str = "", config_path: str = "", 
                   memory_efficient: bool = False, cpu_offload: bool = False) -> Tuple[bool, str]:
        """Load models if not already loaded or if path changed"""
        try:
            # --- New Robust Model Pathing ---
            # Use our model manager as a pre-flight check to ensure all models
            # are downloaded and present before we proceed.
            logger.info("Verifying local model integrity...")
            find_or_download("hunyuanvideo_foley.pth", "Tencent-Hunyuan/HunyuanVideo-Foley", "hunyuanvideo-foley-xxl")
            find_or_download("vae_128d_48k.pth", "Tencent-Hunyuan/HunyuanVideo-Foley", "hunyuanvideo-foley-xxl")
            find_or_download("synchformer_state_dict.pth", "Tencent-Hunyuan/HunyuanVideo-Foley", "hunyuanvideo-foley-xxl")
            get_siglip_path()  # This ensures the Hugging Face cache is populated if needed
            get_clap_path()
            logger.info("All models found or downloaded successfully.")

            # Now, get the single directory path that the original library expects.
            model_dir = get_model_dir("hunyuanvideo-foley-xxl")

            if not config_path.strip():
                current_dir = os.path.dirname(os.path.abspath(__file__))
                config_path = os.path.join(current_dir, "configs", "hunyuanvideo-foley-xxl.yaml")

            # --- Caching Check (checks against the directory path string) ---
            if (cls._model_dict is not None and 
                cls._cfg is not None and 
                (cls._model_path == model_dir or cls._model_path == "preloaded") and
                cls._memory_efficient == memory_efficient):
                return True, "Models already loaded"
            
            cls._device = cls.setup_device("auto", 0)
            logger.info(f"Loading models from directory: {model_dir}")
            logger.info(f"Config: {config_path}")
            
            # Call the original library loader with the directory path it expects.
            cls._model_dict, cls._cfg = load_model(model_dir, config_path, cls._device)
            
            if cls._model_path != "preloaded":
                cls._model_path = model_dir
            cls._memory_efficient = memory_efficient
            
            logger.info("Models loaded successfully!")
            return True, "Models loaded successfully!"
        
        except Exception as e:
            import traceback
            error_msg = f"Failed to load models: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            cls._model_dict = None
            cls._cfg = None
            cls._device = None
            cls._model_path = None
            return False, error_msg

    def set_seed(self, seed: int):
        """Set random seed for reproducibility"""
        # Clamp seed to valid range for numpy (0 to 2^32-1)
        seed = int(seed) % (2**32)

        torch.manual_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)


    @classmethod
    def _extract_frames_from_image_input(cls, images, fps=24.0):
        """Convert IMAGE input to video frames and create a temporary video file"""
        import cv2

        try:
            if images is None:
                return None, "No images provided"

            # Handle different image input formats
            if hasattr(images, 'shape'):
                # Tensor input [batch, height, width, channels]
                if len(images.shape) == 4:
                    frames = images.cpu().numpy()
                else:
                    return None, f"Unexpected image tensor shape: {images.shape}"
            else:
                return None, f"Unsupported image input type: {type(images)}"

            # --- Data Type and Shape Correction ---
            if frames.dtype != np.uint8:
                if frames.max() <= 1.0:
                    frames = (frames * 255)
                frames = frames.astype(np.uint8)
            
            # Ensure 3 channels (RGB) for video writing
            if frames.shape[-1] == 4:
                frames = frames[..., :3]
            # --- End of Fix ---

            # Get dimensions
            batch_size, height, width = frames.shape[:3]

            # Create temporary video file
            temp_fd, temp_path = tempfile.mkstemp(suffix='.mp4')
            os.close(temp_fd)

            # Setup video writer
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_path, fourcc, fps, (width, height))

            if not out.isOpened():
                return None, "Failed to open video writer"

            # Write frames
            for i in range(batch_size):
                frame = frames[i]

                # Handle channels - convert RGB to BGR for OpenCV
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                elif len(frame.shape) == 3 and frame.shape[2] == 4:
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)

                out.write(frame)

            out.release()

            logger.info(f"Created temporary video from {batch_size} frames at {fps} FPS: {temp_path}")
            return temp_path, "Success"

        except Exception as e:
            return None, f"Error converting images to video: {str(e)}"

    # In HunyuanVideoFoleyNode class:
    @torch.inference_mode()
    def generate_audio(self, text_prompt: str, guidance_scale: float,
                      num_inference_steps: int, sample_nums: int, seed: int,
                      video=None, images=None, fps=24.0,
                      negative_prompt="",
                      output_format="video_path",
                      output_folder: str = "hunyuan_foley",
                      filename_prefix: str = "foley_",
                      memory_efficient: bool = False,
                      cpu_offload: bool = False,
                      enabled: bool = True,
                      silent_audio: bool = True,
                      feature_extraction_batch_size: int = 16,
                      syncformer_batch_size: int = 8,
                      enable_profiling: bool = False):
        """Generate audio for the input video/images with the given text prompt"""
        
        # --- Input Validation & Auto Batching ---
        effective_batch_size = feature_extraction_batch_size
        if effective_batch_size == 0:
            from .utils import get_auto_batch_size
            effective_batch_size = get_auto_batch_size()

        if effective_batch_size < 1:
            logger.warning(f"Batch size cannot be less than 1. Clamping value from {feature_extraction_batch_size} to 1.")
            effective_batch_size = 1
            
        if not enabled:
            logger.info("HunyuanVideo-Foley node is disabled. Passing through inputs.")
            return create_node_exit_values(
                silent_audio=silent_audio,
                passthrough_video=video,
                passthrough_images=images,
                message="✅ Node disabled. Skipped."
            )

        try:
            self.set_seed(seed)

            # --- Proactive VRAM Cleanup ---
            logger.info("Performing pre-run VRAM cleanup...")
            mm.unload_all_models()
            import gc; gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            # --- End of Fix ---

            if self._model_dict is None or self._cfg is None:
                logger.info("No models loaded, attempting to load fresh...")
                success, message = self.load_models(memory_efficient=memory_efficient, cpu_offload=cpu_offload)
                if not success:
                    raise Exception(f"Model loading failed: {message}")
            else:
                logger.info("Using pre-loaded models from class state.")

            if self._model_dict is None or self._cfg is None:
                raise Exception("Models not loaded")

            if video is None and images is None:
                raise Exception("Please provide either video or images input!")

            logger.info("Processing media features...")
            visual_feats, text_feats, audio_len_in_s = feature_process_unified(
                video_input=video,
                image_input=images,
                prompt=text_prompt,
                negative_prompt=negative_prompt,
                model_dict=self._model_dict,
                cfg=self._cfg,
                fps_hint=fps,
                batch_size=effective_batch_size,
                sync_batch_size=syncformer_batch_size,
                enable_profiling=enable_profiling
            )

            # --- State Correction and VRAM Management for Denoising ---
            target_device = self._device
            logger.info(f"Preparing for denoising on device: {target_device}")

            visual_feats['siglip2_feat'] = visual_feats['siglip2_feat'].to("cpu")
            visual_feats['syncformer_feat'] = visual_feats['syncformer_feat'].to("cpu")
            text_feats['text_feat'] = text_feats['text_feat'].to("cpu")
            text_feats['uncond_text_feat'] = text_feats['uncond_text_feat'].to("cpu")
            logger.info("Feature tensors moved to CPU.")

            self._model_dict.foley_model.to(target_device)
            self._model_dict.dac_model.to(target_device)
            logger.info("Core models moved to GPU.")
            # --- End of Fix ---

            logger.info("Generating audio...")
            audio, sample_rate = denoise_process_safely(
                visual_feats, text_feats, audio_len_in_s, self._model_dict, self._cfg,
                guidance_scale=guidance_scale, num_inference_steps=num_inference_steps, batch_size=sample_nums
            )

            output_dir = os.path.join(folder_paths.get_output_directory(), output_folder)
            os.makedirs(output_dir, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            audio_filename = f"{filename_prefix}audio_{timestamp}_{seed}.wav"
            audio_output_path = os.path.join(output_dir, audio_filename)
            torchaudio.save(audio_output_path, audio[0], sample_rate)

            audio_tensor = audio[0].unsqueeze(0)
            if len(audio_tensor.shape) == 2:
                audio_tensor = audio_tensor.unsqueeze(1)
            audio_result = {"waveform": audio_tensor, "sample_rate": sample_rate}

            video_file = None
            temp_video_created = False

            if video is not None:
                video_file = extract_video_path(video)
            elif images is not None:
                video_file, _ = self._extract_frames_from_image_input(images, fps)
                temp_video_created = True

            if video_file is None:
                logger.warning("No valid video file path found for output merging/frame extraction.")

            video_output_path = ""
            video_frames = torch.zeros((1, 1, 1, 3), dtype=torch.float32)

            if output_format in ["video_path", "both"] and video_file:
                video_filename = f"{filename_prefix}video_{timestamp}_{seed}.mp4"
                video_output_path = os.path.join(output_dir, video_filename)
                try:
                    merge_audio_video(audio_output_path, video_file, video_output_path)
                    logger.info(f"Created video with audio: {video_output_path}")
                except Exception as e:
                    logger.error(f"Failed to merge audio and video: {e}")
                    video_output_path = ""

            if output_format in ["frames", "both"]:
                if images is not None:
                    video_frames = images
                elif video_file:
                    try:
                        import cv2
                        cap = cv2.VideoCapture(video_file)
                        frames_list = []
                        while True:
                            ret, frame = cap.read()
                            if not ret: break
                            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                            frames_list.append(np.array(frame_rgb, dtype=np.float32) / 255.0)
                        cap.release()
                        if frames_list:
                            video_frames = torch.from_numpy(np.stack(frames_list))
                            logger.info(f"Extracted {len(frames_list)} frames for output")
                    except Exception as e:
                        logger.warning(f"Could not extract frames for output: {e}")

            if temp_video_created and os.path.exists(video_file):
                try: os.remove(video_file)
                except: pass

            success_msg = f"✅ Generated audio successfully"
            return (video_output_path, video_frames, audio_result, success_msg)

        except Exception as e:
            import traceback
            error_msg = f"❌ Generation failed: {str(e)}"
            logger.error(error_msg); logger.error(traceback.format_exc())
            return create_node_exit_values(
                silent_audio=silent_audio,
                passthrough_video=video,
                passthrough_images=images,
                message=error_msg
            )
        
        finally:
            logger.info("HunyuanVideo-Foley: Starting guaranteed cleanup...")
            # If cpu_offload is on, always move models to CPU.
            if self._model_dict and cpu_offload:
                logger.info("Offloading models to CPU...")
                for model_name, model_obj in self._model_dict.items():
                    if hasattr(model_obj, 'to'):
                        try: model_obj.to("cpu")
                        except Exception: pass
            
            # If memory_efficient is on AND the model was loaded by this node (not preloaded),
            # then unload it completely.
            if memory_efficient and self._model_path != "preloaded":
                logger.info("Memory efficient mode: Unloading models completely.")
                self._model_dict = None
                self._cfg = None
                self._model_path = None

            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.info("HunyuanVideo-Foley: Cleanup complete.")

class LinearFP8Wrapper(nn.Module):
    """FP8 quantization wrapper for linear layers"""
    def __init__(self, original_linear, dtype="fp8_e4m3fn"):
        super().__init__()
        self.dtype = dtype
        self.weight_shape = original_linear.weight.shape
        self.bias = original_linear.bias

        # Quantize weights to FP8
        if dtype == "fp8_e4m3fn":
            self.weight_fp8 = original_linear.weight.to(torch.float8_e4m3fn)
        elif dtype == "fp8_e5m2":
            self.weight_fp8 = original_linear.weight.to(torch.float8_e5m2)
        else:
            self.weight_fp8 = original_linear.weight

    def forward(self, x):
        # Convert back to computation dtype for matmul
        weight = self.weight_fp8.to(x.dtype)
        return F.linear(x, weight, self.bias)

class HunyuanVideoFoleyModelLoader:
    """Separate model loader with FP8 quantization support"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "quantization": (["none", "fp8_e4m3fn", "fp8_e5m2"], {
                    "default": "none",
                    "tooltip": "FP8 weight-only quantization for VRAM savings"
                }),
                "cpu_offload": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "Offload models to CPU when not in use"
                }),
            },
            "optional": {
                "model_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "Path to model weights (leave empty for default)"
                }),
                "config_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "Path to config file (leave empty for default)"
                }),
            }
        }

    RETURN_TYPES = ("FOLEY_MODEL", "STRING")
    RETURN_NAMES = ("model", "status")
    FUNCTION = "load_model"
    CATEGORY = "HunyuanVideo-Foley/Loaders"
    DESCRIPTION = "Load HunyuanVideo-Foley model with optional FP8 quantization"

    def load_model(self, quantization="none", cpu_offload=False, model_path="", config_path=""):
        try:
            # Set default paths
            if not model_path or not model_path.strip():
                foley_models_dir = folder_paths.folder_names_and_paths.get("foley", [None])[0]
                if foley_models_dir and len(foley_models_dir) > 0:
                    # Check for the model in the hunyuanvideo-foley-xxl subdirectory
                    model_path = os.path.join(foley_models_dir[0], "hunyuanvideo-foley-xxl")
                else:
                    current_dir = os.path.dirname(os.path.abspath(__file__))
                    model_path = os.path.join(current_dir, "pretrained_models")

            if not config_path or not config_path.strip():
                current_dir = os.path.dirname(os.path.abspath(__file__))
                config_path = os.path.join(current_dir, "configs", "hunyuanvideo-foley-xxl.yaml")

            logger.info(f"Model path: {model_path}")
            logger.info(f"Config path: {config_path}")

            # Setup device
            device = mm.get_torch_device()

            logger.info(f"Loading model with quantization={quantization}")

            # Load model and config
            model_dict, cfg = load_model(model_path, config_path, device)

            # Apply FP8 quantization if requested
            if quantization != "none" and "vae" in model_dict:
                logger.info(f"Applying {quantization} quantization to VAE...")
                vae_model = model_dict["vae"]

                # Quantize linear layers in VAE
                for name, module in vae_model.named_modules():
                    if isinstance(module, nn.Linear):
                        # Replace with FP8 wrapper
                        parent_name = ".".join(name.split(".")[:-1]) if "." in name else ""
                        child_name = name.split(".")[-1]
                        parent = vae_model if not parent_name else dict(vae_model.named_modules())[parent_name]
                        setattr(parent, child_name, LinearFP8Wrapper(module, quantization))

                logger.info("FP8 quantization applied")

            # Package model info
            model_info = {
                "model_dict": model_dict,
                "cfg": cfg,
                "device": device,
                "quantization": quantization,
                "cpu_offload": cpu_offload
            }

            status = f"✅ Model loaded with {quantization} quantization" if quantization != "none" else "✅ Model loaded"
            return (model_info, status)

        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            return (None, f"❌ Failed to load model: {str(e)}")

class HunyuanVideoFoleyDependenciesLoader:
    """Load text encoder and feature extractors separately"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("FOLEY_MODEL",),
                "load_text_encoder": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Load CLAP text encoder"
                }),
                "load_feature_extractor": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Load visual feature extractor"
                }),
            }
        }

    RETURN_TYPES = ("FOLEY_DEPS", "STRING")
    RETURN_NAMES = ("dependencies", "status")
    FUNCTION = "load_dependencies"
    CATEGORY = "HunyuanVideo-Foley/Loaders"
    DESCRIPTION = "Load model dependencies (text encoder, feature extractors)"

    def load_dependencies(self, model, load_text_encoder=True, load_feature_extractor=True):
        try:
            if model is None:
                return (None, "❌ No model provided - check ModelLoader output")

            # Check if model is a tuple (from failed load)
            if isinstance(model, tuple):
                model = model[0]

            if model is None or not isinstance(model, dict):
                return (None, "❌ Invalid model input - ModelLoader may have failed")

            deps = {
                "model_dict": model["model_dict"],
                "cfg": model["cfg"],
                "device": model["device"]
            }

            status_parts = []

            if load_text_encoder:
                logger.info("Loading text encoder...")
                # Text encoder is already in model_dict
                status_parts.append("text encoder")

            if load_feature_extractor:
                logger.info("Loading feature extractor...")
                # Feature extractor is already in model_dict
                status_parts.append("feature extractor")

            status = f"✅ Loaded: {', '.join(status_parts)}" if status_parts else "✅ Dependencies ready"
            return (deps, status)

        except Exception as e:
            logger.error(f"Failed to load dependencies: {e}")
            return (None, f"❌ Failed: {str(e)}")

class HunyuanVideoFoleyTorchCompile:
    """Apply torch.compile optimization to the model"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "dependencies": ("FOLEY_DEPS",),
                "compile_vae": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Compile VAE with torch.compile for ~30% speedup"
                }),
                "compile_mode": (["default", "reduce-overhead", "max-autotune"], {
                    "default": "default",
                    "tooltip": "Compilation mode (default is fastest compile time)"
                }),
                "backend": (["inductor", "cudagraphs", "eager"], {
                    "default": "inductor",
                    "tooltip": "Compilation backend"
                }),
            }
        }

    RETURN_TYPES = ("FOLEY_COMPILED", "STRING")
    RETURN_NAMES = ("compiled_model", "status")
    FUNCTION = "compile_model"
    CATEGORY = "HunyuanVideo-Foley/Optimization"
    DESCRIPTION = "Optimize model with torch.compile for faster inference"

    def compile_model(self, dependencies, compile_vae=True, compile_mode="default", backend="inductor"):
        try:
            if dependencies is None:
                return (None, "❌ No dependencies provided - check DependenciesLoader output")

            # Check if dependencies is a tuple (from failed load)
            if isinstance(dependencies, tuple):
                dependencies = dependencies[0]

            if dependencies is None or not isinstance(dependencies, dict):
                return (None, "❌ Invalid dependencies - previous node may have failed")

            # Handle AttributeDict or regular dict
            model_dict = dependencies["model_dict"]
            # Don't copy AttributeDict, just reference it
            if hasattr(model_dict, '__class__') and 'AttributeDict' in str(model_dict.__class__):
                model_dict_ref = model_dict
            else:
                model_dict_ref = model_dict.copy() if hasattr(model_dict, 'copy') else model_dict

            compiled = {
                "model_dict": model_dict_ref,
                "cfg": dependencies["cfg"],
                "device": dependencies["device"],
                "compiled": False
            }

            # Check for dac_model (the actual VAE used in HunyuanVideo-Foley)
            model_dict = compiled["model_dict"]
            has_dac = hasattr(model_dict, 'dac_model') or (isinstance(model_dict, dict) and "dac_model" in model_dict)

            if compile_vae and has_dac:
                logger.info(f"Compiling DAC VAE with mode={compile_mode}, backend={backend}...")

                import torch._dynamo as dynamo
                dynamo.config.suppress_errors = True

                # Only compile if backend is not eager
                if backend != "eager":
                    # Access dac_model correctly whether it's dict or AttributeDict
                    if hasattr(model_dict, 'dac_model'):
                        dac_model = model_dict.dac_model
                    else:
                        dac_model = model_dict["dac_model"]

                    compiled_dac = torch.compile(
                        dac_model,
                        mode=compile_mode,
                        backend=backend
                    )

                    # Set the compiled model back
                    if hasattr(model_dict, 'dac_model'):
                        model_dict.dac_model = compiled_dac
                    else:
                        model_dict["dac_model"] = compiled_dac

                    compiled["compiled"] = True
                    status = f"✅ DAC VAE compiled with {compile_mode}/{backend}"
                else:
                    status = "✅ Model ready (eager mode - no compilation)"
            else:
                status = "✅ Model ready (no VAE compilation)"

            return (compiled, status)

        except Exception as e:
            logger.error(f"Failed to compile model: {e}")
            # Return uncompiled model on failure
            if dependencies:
                return (dependencies, f"⚠️ Compilation failed, using uncompiled: {str(e)}")
            return (None, f"❌ Failed: {str(e)}")

class HunyuanVideoFoleyGeneratorAdvanced(HunyuanVideoFoleyNode):
    """Enhanced generator that can use separately loaded models"""

    @classmethod
    def INPUT_TYPES(cls):
        base_inputs = super().INPUT_TYPES()

        # Add optional compiled model input
        base_inputs["optional"]["compiled_model"] = ("FOLEY_COMPILED",)
        # --- QoL FIELDS HERE ---
        base_inputs["optional"]["enabled"] = ("BOOLEAN", {
            "default": True,
            "display": "checkbox",
            "tooltip": "Enable or disable this node. If disabled, it will pass through silent audio."
        })
        base_inputs["optional"]["silent_audio"] = ("BOOLEAN", {
            "default": True,
            "display": "checkbox",
            "tooltip": "If true, returns a silent audio clip when disabled or on failure. If false, returns None."
        })
        # -----------------------------

        return base_inputs

    FUNCTION = "generate_audio_advanced"
    CATEGORY = "HunyuanVideo-Foley"
    DESCRIPTION = "Generate audio with optional pre-loaded/optimized models"

    def generate_audio_advanced(self, **kwargs):
        compiled_model = kwargs.get("compiled_model")

        if compiled_model:
            if isinstance(compiled_model, tuple): compiled_model = compiled_model[0]
            if compiled_model and isinstance(compiled_model, dict):
                self.__class__._model_dict = compiled_model.get("model_dict")
                self.__class__._cfg = compiled_model.get("cfg")
                self.__class__._device = compiled_model.get("device")
                self.__class__._model_path = "preloaded"
                self.__class__._memory_efficient = kwargs.get("memory_efficient", False)
                logger.info("Using pre-loaded/compiled model from input.")
            else:
                logger.warning("Invalid compiled_model input. Clearing state to force reload.")
                self.__class__._model_dict, self.__class__._cfg, self.__class__._model_path = None, None, None
        else:
            logger.info("No pre-loaded model detected. Clearing state to force fresh load.")
            self.__class__._model_dict, self.__class__._cfg, self.__class__._model_path = None, None, None
        
        # Pop the compiled_model so it's not passed to the parent
        kwargs.pop("compiled_model", None)

        return self.generate_audio(**kwargs)

# Node mappings for ComfyUI
NODE_CLASS_MAPPINGS = {
    "HunyuanVideoFoley": HunyuanVideoFoleyNode,
    "HunyuanVideoFoleyModelLoader": HunyuanVideoFoleyModelLoader,
    "HunyuanVideoFoleyDependenciesLoader": HunyuanVideoFoleyDependenciesLoader,
    "HunyuanVideoFoleyTorchCompile": HunyuanVideoFoleyTorchCompile,
    "HunyuanVideoFoleyGeneratorAdvanced": HunyuanVideoFoleyGeneratorAdvanced,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "HunyuanVideoFoley": "HunyuanVideo-Foley Generator",
    "HunyuanVideoFoleyModelLoader": "HunyuanVideo-Foley Model Loader (FP8)",
    "HunyuanVideoFoleyDependenciesLoader": "HunyuanVideo-Foley Dependencies",
    "HunyuanVideoFoleyTorchCompile": "HunyuanVideo-Foley Torch Compile",
    "HunyuanVideoFoleyGeneratorAdvanced": "HunyuanVideo-Foley Generator (Advanced)",
}